<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[[kaggle]Digit Recognizer(acc 99.61%, top 10%)]]></title>
    <url>%2F2018%2F07%2F05%2Fkaggle-Digit-Recognizer%2F</url>
    <content type="text"><![CDATA[IntroductionThis project is a solution of the kaggle contest - Digit Recognizer, which aims at recognizing single hand-writing digit. This solution is based on CNN and transfer-learning, resulting in accuracy of 99.61% (with transfer-learning),as a score of top 10%, and 99.40% (without transfer-learning). I believe that this is one of the best solutions without over-fitting or more relative data, such as the EMNIST dataset and even the MNIST dataset. Considering the test data, which comes from MNIST, is public and labeled, the models of 99.9%+ solutions could possibly be over-fitting to the MINST dataset. To avoid over-fitting, I used lots of dropout-layers and batchnorm-layers. Also, I submit my solution only one time for each model. You can find the whole ipynb code in my github. EnvironmentJupyter NotebookUbuntu 16.04.4GeForce GTX 1080 Tipython==3.5keras==2.2.0tensorflow-gpu==1.8.0 I strongly suggest you use the latest published version of keras, or make sure itâ€™s later than version 2.1.2, because the earlier version used different functions and some bugs existed (such as bug functions preprocessing_function = preprocess_input, and flow_from_directory)Jupyter notebook is recommended since it helps you to debug, check intermediate variables, and visualization.Additionally, I suggest you use GPU(s) instead of CPU, for the former is much faster in training. CNN without transfer-learning (for beginners)Features vgg-liked architecture data augmentation visualization data generator confusion matrix Data Pre-processingImport packages and set paths.123456789101112131415import numpy as npimport matplotlib.pyplot as plt%matplotlib inlinefrom keras.models import Sequentialfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalizationfrom keras.optimizers import Adamfrom keras.utils.np_utils import to_categorical # convert to one-hot-encodingfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import confusion_matrixfrom keras.preprocessing.image import ImageDataGeneratorfrom keras.callbacks import LearningRateSchedulertrain_file = "./input/train.csv"test_file = "./input/test.csv"output_file = "./output/submission.csv" Load the data and split train/dev.12raw_data = np.loadtxt(train_file, skiprows=1, dtype='int', delimiter=',')x_train, x_dev, y_train, y_dev = train_test_split(raw_data[:,1:], raw_data[:,0], test_size=0.1,random_state=0) Visualize the data.12345fig, ax = plt.subplots(2, 1, figsize=(12,6))ax[0].plot(x_train[0])ax[0].set_title('784x1 data')ax[1].imshow(x_train[0].reshape(28,28),cmap='gray')ax[1].set_title('28x28 data') Normalize the data from [0,255] to [1, 0].123456x_train = x_train.reshape(-1,28,28,1)x_dev = x_dev.reshape(-1,28,28,1)x_train = x_train.astype("float32")/255.0x_dev = x_dev.astype("float32")/255.0y_train = to_categorical(y_train)y_dev = to_categorical(y_dev) Define the modelInspired by vgg, I used a 11-layer sequential model as a small version of vgg.In fact, vgg is too large for the competition, because it is hard to train, and reducing its size will not hurt accuracy.12345678910111213141516171819202122232425model = Sequential()model.add(Conv2D(filters = 32, kernel_size = (5, 5),padding='same', activation='relu',input_shape = (28, 28, 1)))model.add(BatchNormalization())model.add(Conv2D(filters = 32, kernel_size = (5, 5),padding='same', activation='relu'))model.add(BatchNormalization())model.add(Dropout(0.25))model.add(MaxPool2D((2, 2), strides=(2, 2)))model.add(Conv2D(filters = 64, kernel_size = (3, 3),padding='same', activation='relu'))model.add(BatchNormalization())model.add(Conv2D(filters = 64, kernel_size = (3, 3),padding='same', activation='relu'))model.add(BatchNormalization())model.add(Dropout(0.25))model.add(MaxPool2D((2, 2), strides=(2, 2)))model.add(Flatten())model.add(Dense(2048, activation='relu'))model.add(Dropout(0.25))model.add(Dense(512, activation='relu'))model.add(Dropout(0.25))model.add(Dense(128, activation='relu'))model.add(Dropout(0.25))model.add(Dense(10, activation='softmax'))annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x) The original training set is relatively small, so you need to augment it for more data.1234augmented_data_generator = ImageDataGenerator(zoom_range = 0.1, height_shift_range = 0.1, width_shift_range = 0.1, rotation_range = 10) Compile and train the modelI used 9 minutes to train 30 epochs.1234567model.compile(loss='categorical_crossentropy', optimizer = Adam(lr=1e-4), metrics=["accuracy"])hist = model.fit_generator(augmented_data_generator.flow(x_train, y_train, batch_size=64), steps_per_epoch=1024, epochs=30, verbose=1, # 0 = silent, 1 = progress bar, 2 = one line per epoch. validation_data=(x_dev, y_dev), callbacks=[annealer]) Check the training processPrint loss and accuracy curves.12final_loss, final_acc = model.evaluate(x_dev, y_dev, verbose=0)print("Final loss: &#123;0:.4f&#125;, final accuracy: &#123;1:.4f&#125;".format(final_loss, final_acc)) Print confusion matrix.12345y_hat = model.predict(x_dev)y_pred = np.argmax(y_hat, axis=1)y_true = np.argmax(y_dev, axis=1)cm = confusion_matrix(y_true, y_pred)print(cm) Predict and save the result123456789mnist_testset = np.loadtxt(test_file, skiprows=1, dtype='int', delimiter=',')x_test = mnist_testset.astype("float32")x_test = x_test.reshape(-1, 28, 28, 1)/255.y_hat = model.predict(x_test, batch_size=64)y_pred = np.argmax(y_hat,axis=1)with open(output_file, 'w') as f : f.write('ImageId,Label\n') for i in range(len(y_pred)) : f.write("".join([str(i+1),',',str(y_pred[i]),'\n'])) CNN with transfer-learningFeatures inception-v3 architecture freeze layers data augmentation visualization data generator test generator You may find the code of this part in my github. ReferenceWelcome to deep learning (CNN 99%) @FlouriteJ2018-07-05]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>keras</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
</search>
